{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Enhancement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from nlppets.torch import (\n",
    "    count_parameters,\n",
    "    nested_freeze_tensor,\n",
    "    count_trainable_parameters,\n",
    ")\n",
    "from nlppets.transformers.model.bert import domain_enhance_att, domain_enhance_ffn\n",
    "\n",
    "model = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "# if using fp tuning\n",
    "model = domain_enhance_ffn(model, {\"domain_name\": 1024})\n",
    "# if using att enhancement\n",
    "model = domain_enhance_att(model, {\"domain_name\": 4})\n",
    "\n",
    "# freeze tensors\n",
    "ENHANCEMENTS = {\"domain_name\"}\n",
    "model = nested_freeze_tensor(\n",
    "    model,\n",
    "    exclude={\n",
    "        # if using fp tuning\n",
    "        *(f\"bert.encoder.layer.*.intermediate.{e}.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"bert.encoder.layer.*.output.{e}.*\" for e in ENHANCEMENTS),\n",
    "        # if using att enhancement\n",
    "        *(f\"bert.encoder.layer.*.attention.self.{e}_query.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"bert.encoder.layer.*.attention.self.{e}_key.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"bert.encoder.layer.*.attention.self.{e}_value.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"bert.encoder.layer.*.attention.output.{e}.*\" for e in ENHANCEMENTS),\n",
    "    },\n",
    ")\n",
    "\n",
    "params = count_parameters(model)\n",
    "trainable = count_trainable_parameters(model)\n",
    "print(\n",
    "    \"Parameters:\",\n",
    "    params,\n",
    "    \"Trainable:\",\n",
    "    trainable,\n",
    "    \"Trainable/Parameters:\",\n",
    "    f\"{trainable/params:.2%}\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example ChatGLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from nlppets.torch import (\n",
    "    count_parameters,\n",
    "    nested_freeze_tensor,\n",
    "    count_trainable_parameters,\n",
    ")\n",
    "from nlppets.transformers.model.chatglm import domain_enhance_att, domain_enhance_ffn\n",
    "\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\")\n",
    "# if using fp tuning\n",
    "model = domain_enhance_ffn(model, {\"domain_name\": 1024})\n",
    "# if using att enhancement\n",
    "model = domain_enhance_att(model, {\"domain_name\": 4})\n",
    "\n",
    "# freeze tensors\n",
    "ENHANCEMENTS = {\"domain_name\"}\n",
    "model = nested_freeze_tensor(\n",
    "    model,\n",
    "    exclude={\n",
    "        # if using fp tuning\n",
    "        *(f\"transformer.layers.*.mlp.{e}_up.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"transformer.layers.*.mlp.{e}_down.*\" for e in ENHANCEMENTS),\n",
    "        # if using att enhancement\n",
    "        *(f\"transformer.layers.*.attention.{e}.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"transformer.layers.*.attention.{e}_output.*\" for e in ENHANCEMENTS),\n",
    "    },\n",
    ")\n",
    "\n",
    "params = count_parameters(model)\n",
    "trainable = count_trainable_parameters(model)\n",
    "print(\n",
    "    \"Parameters:\",\n",
    "    params,\n",
    "    \"Trainable:\",\n",
    "    trainable,\n",
    "    \"Trainable/Parameters:\",\n",
    "    f\"{trainable/params:.2%}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
