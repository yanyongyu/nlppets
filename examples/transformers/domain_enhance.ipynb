{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Enhancement\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example BERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from nlppets.torch import (\n",
    "    count_parameters,\n",
    "    nested_freeze_tensor,\n",
    "    count_trainable_parameters,\n",
    ")\n",
    "from nlppets.transformers.model.bert import domain_enhance_att, domain_enhance_ffn\n",
    "\n",
    "model = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "# if using fp tuning\n",
    "model = domain_enhance_ffn(model, {\"domain_name\": 1024})\n",
    "# if using att enhancement\n",
    "model = domain_enhance_att(model, {\"domain_name\": 4})\n",
    "\n",
    "# freeze tensors\n",
    "ENHANCEMENTS = {\"domain_name\"}\n",
    "model = nested_freeze_tensor(\n",
    "    model,\n",
    "    exclude={\n",
    "        # if using fp tuning\n",
    "        *(f\"bert.encoder.layer.*.intermediate.{e}.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"bert.encoder.layer.*.output.{e}.*\" for e in ENHANCEMENTS),\n",
    "        # if using att enhancement\n",
    "        *(f\"bert.encoder.layer.*.attention.self.{e}_query.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"bert.encoder.layer.*.attention.self.{e}_key.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"bert.encoder.layer.*.attention.self.{e}_value.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"bert.encoder.layer.*.attention.output.{e}.*\" for e in ENHANCEMENTS),\n",
    "    },\n",
    ")\n",
    "\n",
    "params = count_parameters(model)\n",
    "trainable = count_trainable_parameters(model)\n",
    "print(\n",
    "    \"Parameters:\",\n",
    "    params,\n",
    "    \"Trainable:\",\n",
    "    trainable,\n",
    "    \"Trainable/Parameters:\",\n",
    "    f\"{trainable/params:.2%}\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example ChatGLM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from typing import Type\n",
    "from transformers import AutoConfig, PreTrainedModel\n",
    "from transformers.dynamic_module_utils import get_class_from_dynamic_module\n",
    "from nlppets.torch import (\n",
    "    count_parameters,\n",
    "    nested_freeze_tensor,\n",
    "    count_trainable_parameters,\n",
    ")\n",
    "from nlppets.transformers.model.chatglm import domain_enhance_att, domain_enhance_ffn\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "class_ref = config.auto_map[\"AutoModel\"]\n",
    "ChatGLMForConditionalGeneration: Type[PreTrainedModel] = get_class_from_dynamic_module(\n",
    "    class_ref, \"THUDM/chatglm-6b\"\n",
    ")\n",
    "\n",
    "# if using fp tuning\n",
    "ChatGLMForConditionalGeneration = domain_enhance_ffn(\n",
    "    ChatGLMForConditionalGeneration, {\"domain_name\": 1024}\n",
    ")\n",
    "# if using att enhancement\n",
    "ChatGLMForConditionalGeneration = domain_enhance_att(\n",
    "    ChatGLMForConditionalGeneration, {\"domain_name\": 4}\n",
    ")\n",
    "\n",
    "model: PreTrainedModel = ChatGLMForConditionalGeneration.from_pretrained(\n",
    "    \"THUDM/chatglm-6b\", trust_remote_code=True\n",
    ")  # type: ignore\n",
    "\n",
    "# freeze tensors\n",
    "ENHANCEMENTS = {\"domain_name\"}\n",
    "model = nested_freeze_tensor(\n",
    "    model,\n",
    "    exclude={\n",
    "        # if using fp tuning\n",
    "        *(f\"transformer.layers.*.mlp.{e}_up.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"transformer.layers.*.mlp.{e}_down.*\" for e in ENHANCEMENTS),\n",
    "        # if using att enhancement\n",
    "        *(f\"transformer.layers.*.attention.{e}.*\" for e in ENHANCEMENTS),\n",
    "        *(f\"transformer.layers.*.attention.{e}_output.*\" for e in ENHANCEMENTS),\n",
    "    },\n",
    ")\n",
    "\n",
    "params = count_parameters(model)\n",
    "trainable = count_trainable_parameters(model)\n",
    "print(\n",
    "    \"Parameters:\",\n",
    "    params,\n",
    "    \"Trainable:\",\n",
    "    trainable,\n",
    "    \"Trainable/Parameters:\",\n",
    "    f\"{trainable/params:.2%}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
